import streamlit as st
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.groq import Groq
from llama_index.embeddings.huggingface.base import HuggingFaceEmbedding

# ============================
# PAGE CONFIG
# ============================
st.set_page_config(page_title="RAG Chatbot PDF", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ RAG Chatbot PDF ‚Äì Streamlit Cloud + Groq (FREE)")

# ============================
# UPLOAD PDF
# ============================
st.sidebar.header("üìÑ Upload PDF")
uploaded_files = st.sidebar.file_uploader(
    "T·∫£i l√™n file PDF",
    type=["pdf"],
    accept_multiple_files=True
)

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# L∆∞u file v√†o th∆∞ m·ª•c data/
if uploaded_files:
    for file in uploaded_files:
        file_path = os.path.join(DATA_DIR, file.name)
        with open(file_path, "wb") as f:
            f.write(file.getbuffer())
    st.sidebar.success("‚úî PDF ƒë√£ l∆∞u v√†o data/")

# ============================
# BUILD INDEX (CACHE)
# ============================
@st.cache_resource
def build_index():
    pdf_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]
    if not pdf_files:
        return None

    st.sidebar.info("üîÑ ƒêang x·ª≠ l√Ω t√†i li·ªáu...")

    # Load PDF th√†nh vƒÉn b·∫£n
    documents = SimpleDirectoryReader(DATA_DIR).load_data()

    # Embedding mi·ªÖn ph√≠ HuggingFace
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # LLM mi·ªÖn ph√≠ c·ªßa Groq ‚Äî MODEL M·ªöI
    llm = Groq(
        model="llama3-8b",   # b·∫°n c√≥ th·ªÉ ƒë·ªïi th√†nh "llama3-70b"
        api_key=st.secrets["GROQ_API_KEY"]
    )

    index = VectorStoreIndex.from_documents(
        documents,
        llm=llm,
        embed_model=embed_model
    )

    return index


index = build_index()

# ============================
# CHAT ENGINE
# ============================
if index:
    llm_chat = Groq(
        model="llama3-8b",  # Ho·∫∑c "llama3-70b" n·∫øu mu·ªën tr·∫£ l·ªùi m·∫°nh h∆°n
        api_key=st.secrets["GROQ_API_KEY"]
    )

    chat_engine = index.as_chat_engine(
        llm=llm_chat,
        chat_mode="condense_question",
        verbose=False
    )
else:
    chat_engine = None

# ============================
# CHAT UI
# ============================
st.subheader("üí¨ Chat v·ªõi PDF")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i
for role, text in st.session_state.messages:
    st.chat_message(role).markdown(text)

# Nh·∫≠p c√¢u h·ªèi
user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi...")

if user_input:
    st.session_state.messages.append(("user", user_input))

    if chat_engine:
        with st.spinner("ü§ñ ƒêang suy nghƒ©..."):
            response = chat_engine.chat(user_input)
            bot_reply = response.response
            st.session_state.messages.append(("assistant", bot_reply))
    else:
        st.session_state.messages.append(
            ("assistant", "‚ö† Vui l√≤ng upload √≠t nh·∫•t 1 PDF."))

    st.rerun()
