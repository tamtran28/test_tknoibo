import streamlit as st
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.groq import Groq
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# ============================
# PAGE CONFIG
# ============================
st.set_page_config(page_title="RAG PDF Chatbot", page_icon="ü§ñ", layout="wide")

st.markdown("""
    <style>
        .block-container { padding-top: 20px; }
        textarea, input {
            background-color: #222 !important;
            color: white !important;
        }
    </style>
""", unsafe_allow_html=True)

st.title("ü§ñ RAG Chatbot PDF (Streamlit Cloud + Groq API ‚Äì FREE)")

# ============================
# SIDEBAR UPLOAD
# ============================
st.sidebar.header("üìÑ Upload PDF")
uploaded_files = st.sidebar.file_uploader(
    "Upload m·ªôt ho·∫∑c nhi·ªÅu file PDF",
    type=["pdf"],
    accept_multiple_files=True
)

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# L∆∞u file PDF v√†o th∆∞ m·ª•c m√°y ch·ªß
if uploaded_files:
    for file in uploaded_files:
        file_path = os.path.join(DATA_DIR, file.name)
        with open(file_path, "wb") as f:
            f.write(file.getbuffer())
    st.sidebar.success("‚úî File ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c data/")

# ============================
# BUILD INDEX (CACHE)
# ============================

@st.cache_resource
def build_index():
    pdf_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]

    if not pdf_files:
        return None

    st.sidebar.info("üîÑ ƒêang load t√†i li·ªáu...")

    # ƒë·ªçc t√†i li·ªáu
    docs = SimpleDirectoryReader(DATA_DIR).load_data()

    # LLM Groq mi·ªÖn ph√≠
    llm = Groq(
        model="llama3-8b-8192",
        api_key=st.secrets["GROQ_API_KEY"]
    )

    # Embedding mi·ªÖn ph√≠ HuggingFace
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # t·∫°o index
    index = VectorStoreIndex.from_documents(
        docs,
        llm=llm,
        embed_model=embed_model
    )
    return index

index = build_index()

# ============================
# CHAT ENGINE
# ============================
if index:
    chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=False)
else:
    chat_engine = None

# ============================
# CHAT UI
# ============================
st.subheader("üí¨ Chat v·ªõi PDF c·ªßa b·∫°n")

if "messages" not in st.session_state:
    st.session_state.messages = []

# hi·ªÉn th·ªã tin nh·∫Øn c≈©
for role, msg in st.session_state.messages:
    st.chat_message(role).markdown(msg)

# input chat
user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi...")

if user_input:
    st.session_state.messages.append(("user", user_input))

    if chat_engine:
        with st.spinner("ü§ñ ƒêang suy nghƒ©..."):
            response = chat_engine.chat(user_input)
            bot_reply = response.response
            st.session_state.messages.append(("assistant", bot_reply))
    else:
        st.session_state.messages.append(("assistant", "‚ö† H√£y upload √≠t nh·∫•t 1 PDF."))

    st.rerun()
